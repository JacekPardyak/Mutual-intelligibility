{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#import tensorflow as tf #\nimport string #\n#import requests #\nimport pandas as pd #\n\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\n# https://en.wikipedia.org/wiki/Pseudoword\n# Source: https://kgptalkie.com/poetry-generation-using-tensorflow-keras-and-lstm/","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#response = requests.get('https://raw.githubusercontent.com/laxmimerit/poetry-data/master/adele.txt')\n#response.text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the Polish wordlist\ntext = str()\nwith open('/kaggle/input/languages-of-europe/pl_PL.csv', encoding = \"iso-8859-2\") as f:\n    text += f.read()\n# check\ntext[:10]    ","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"'a\\nA\\naa\\nAA\\n'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seperate all characters with space\ntext = \" \".join(text)\n# check\ntext[:10]  ","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"'a \\n A \\n a '"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get initial n characters for test\ntext = text[:10000]\n# check\ntext[:10]","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"'a \\n A \\n a '"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split text into lines at each \\n\ndata = text.splitlines()\n# check\ndata[:10]","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"['a ',\n ' A ',\n ' a a ',\n ' A A ',\n ' a a a ',\n ' A a c h e n ',\n ' A a l b o r g ',\n ' A a l t o ',\n ' A A N ',\n ' A A P ']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# that way we have a number of lines which should be number of rows in csv\nlen(data)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"516"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check lenght of text\nlen(\" \".join(data))","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"10000"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vectorize the list of texts\ntoken = Tokenizer()\n# update internal vocabulary based on list of texts\ntoken.fit_on_texts(data)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check word frequency in tokenizer\ntoken.word_counts\n","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"OrderedDict([('a', 966),\n             ('c', 285),\n             ('h', 46),\n             ('e', 223),\n             ('n', 191),\n             ('l', 169),\n             ('b', 437),\n             ('o', 261),\n             ('r', 185),\n             ('g', 33),\n             ('t', 226),\n             ('p', 14),\n             ('d', 85),\n             ('v', 23),\n             ('k', 118),\n             ('u', 143),\n             ('s', 207),\n             ('i', 225),\n             ('j', 105),\n             ('ń', 11),\n             ('z', 89),\n             ('y', 199),\n             ('w', 101),\n             ('ż', 18),\n             ('m', 66),\n             ('ć', 19),\n             ('ą', 17),\n             ('ł', 2),\n             ('f', 8),\n             ('ó', 4),\n             ('x', 2),\n             ('ś', 7)])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check word indices in tokenizer\ntoken.word_index","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"{'a': 1,\n 'b': 2,\n 'c': 3,\n 'o': 4,\n 't': 5,\n 'i': 6,\n 'e': 7,\n 's': 8,\n 'y': 9,\n 'n': 10,\n 'r': 11,\n 'l': 12,\n 'u': 13,\n 'k': 14,\n 'j': 15,\n 'w': 16,\n 'z': 17,\n 'd': 18,\n 'm': 19,\n 'h': 20,\n 'g': 21,\n 'v': 22,\n 'ć': 23,\n 'ż': 24,\n 'ą': 25,\n 'p': 26,\n 'ń': 27,\n 'f': 28,\n 'ś': 29,\n 'ó': 30,\n 'ł': 31,\n 'x': 32}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set size of the vocabulary we will need it for hot encoding target variable\nvocab_size = len(token.word_counts) + 1\nvocab_size","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"33"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode our texts into integers with word indices\nencoded_text = token.texts_to_sequences(data)\n#check\nencoded_text[:10]","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"[[1],\n [1],\n [1, 1],\n [1, 1],\n [1, 1, 1],\n [1, 1, 3, 20, 7, 10],\n [1, 1, 12, 2, 4, 11, 21],\n [1, 1, 12, 5, 4],\n [1, 1, 10],\n [1, 1, 26]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how any text is converted to integers\nx = ['j a c e k']\ntoken.texts_to_sequences(x)","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"[[15, 1, 3, 7, 14]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare Training Data\n# Filter out words of len >1\n# From each word d make left sub words of lenght 2, 3, ..., len(d) - 1\ndatalist = []\nfor d in encoded_text:\n  if len(d)> 1:\n    for i in range(2, len(d)): # ? why we skip the last character?\n      datalist.append(d[:i])\n#      print(d[:i])\n\n#check\ndatalist[:10]","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"[[1, 1],\n [1, 1],\n [1, 1, 3],\n [1, 1, 3, 20],\n [1, 1, 3, 20, 7],\n [1, 1],\n [1, 1, 12],\n [1, 1, 12, 2],\n [1, 1, 12, 2, 4],\n [1, 1, 12, 2, 4, 11]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"datalist = []\nd = encoded_text[5]\nprint(d)\nlen(d)\nif len(d)> 1:\n    for i in range(2, len(d)+1): \n      datalist.append(d[:i])\n#      print(d[:i])\ndatalist","execution_count":15,"outputs":[{"output_type":"stream","text":"[1, 1, 3, 20, 7, 10]\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"[[1, 1], [1, 1, 3], [1, 1, 3, 20], [1, 1, 3, 20, 7], [1, 1, 3, 20, 7, 10]]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"My name  = 'j a c e k' is encoded into integer '15, 1, 3, 7, 14'\n\nThen we make \n\n'15, 1'\n'15, 1, 3'\n'15, 1, 3, 7'\n\nThen we make padding\n\n'0, 0, 0,15, 1'\n'0, 0,15, 1, 3'\n'0,15, 1, 3, 7'\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the new list is naturaly much longer\nlen(datalist)","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"3455"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform our list into 2D Numpy array of shape len(datalist) * max_length (Padding)\nmax_length = 20\nmax_length = 10\nsequences = pad_sequences(datalist, maxlen=max_length, padding='pre')\n#check\nsequences","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"array([[ 0,  0,  0, ...,  0,  1,  1],\n       [ 0,  0,  0, ...,  0,  1,  1],\n       [ 0,  0,  0, ...,  1,  1,  3],\n       ...,\n       [ 0,  0,  1, ...,  4, 16,  6],\n       [ 0,  0,  0, ...,  0,  1,  3],\n       [ 0,  0,  0, ...,  1,  3, 20]], dtype=int32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the hape of the result\nsequences.shape","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"(3455, 10)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"we have only trainong data set\ncolumns without the last are input variables\nlast column is target for classification algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"# abservations and features for classification\nX = sequences[:, :-1]\n# check\nX[:10]","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"array([[ 0,  0,  0,  0,  0,  0,  0,  0,  1],\n       [ 0,  0,  0,  0,  0,  0,  0,  0,  1],\n       [ 0,  0,  0,  0,  0,  0,  0,  1,  1],\n       [ 0,  0,  0,  0,  0,  0,  1,  1,  3],\n       [ 0,  0,  0,  0,  0,  1,  1,  3, 20],\n       [ 0,  0,  0,  0,  0,  0,  0,  0,  1],\n       [ 0,  0,  0,  0,  0,  0,  0,  1,  1],\n       [ 0,  0,  0,  0,  0,  0,  1,  1, 12],\n       [ 0,  0,  0,  0,  0,  1,  1, 12,  2],\n       [ 0,  0,  0,  0,  1,  1, 12,  2,  4]], dtype=int32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target variable converted to factor\ny = sequences[:, -1]\ny = to_categorical(y, num_classes=vocab_size)\n# character in target is hot encoded\n# check\ny[:10]","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this the number of features  we will need it ? \n# we solve  multiclass classification problem\nseq_length = X.shape[1]\nseq_length","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"9"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We initiate stack with sequential layer\nmodel = Sequential()\n# We turn positive integers (indexes) into dense vectors of fixed size\nmodel.add(Embedding(vocab_size, 50, input_length=seq_length))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(LSTM(100))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\n","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the added layers\nmodel.summary()","execution_count":25,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 9, 50)             1650      \n_________________________________________________________________\nlstm (LSTM)                  (None, 9, 100)            60400     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 100)               80400     \n_________________________________________________________________\ndense (Dense)                (None, 100)               10100     \n_________________________________________________________________\ndense_1 (Dense)              (None, 33)                3333      \n=================================================================\nTotal params: 155,883\nTrainable params: 155,883\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We configure the model for training\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# We train the model for a fixed number of epochs (iterations on a dataset)\nmodel.fit(X, y, batch_size=32, epochs=50)","execution_count":29,"outputs":[{"output_type":"stream","text":"Epoch 1/50\n108/108 [==============================] - 6s 22ms/step - loss: 3.1784 - accuracy: 0.1272\nEpoch 2/50\n108/108 [==============================] - 2s 22ms/step - loss: 2.6543 - accuracy: 0.2050\nEpoch 3/50\n108/108 [==============================] - 2s 21ms/step - loss: 2.5911 - accuracy: 0.2049\nEpoch 4/50\n108/108 [==============================] - 2s 20ms/step - loss: 2.5525 - accuracy: 0.2215\nEpoch 5/50\n108/108 [==============================] - 2s 20ms/step - loss: 2.4896 - accuracy: 0.2589\nEpoch 6/50\n108/108 [==============================] - 2s 20ms/step - loss: 2.4807 - accuracy: 0.2539\nEpoch 7/50\n108/108 [==============================] - 2s 19ms/step - loss: 2.4049 - accuracy: 0.2730\nEpoch 8/50\n108/108 [==============================] - 2s 19ms/step - loss: 2.3325 - accuracy: 0.2985\nEpoch 9/50\n108/108 [==============================] - 2s 19ms/step - loss: 2.3052 - accuracy: 0.3110\nEpoch 10/50\n108/108 [==============================] - 2s 21ms/step - loss: 2.2290 - accuracy: 0.3315\nEpoch 11/50\n108/108 [==============================] - 2s 20ms/step - loss: 2.1718 - accuracy: 0.3521\nEpoch 12/50\n108/108 [==============================] - 2s 19ms/step - loss: 2.0479 - accuracy: 0.3850\nEpoch 13/50\n108/108 [==============================] - 2s 19ms/step - loss: 1.9036 - accuracy: 0.4275\nEpoch 14/50\n108/108 [==============================] - 2s 20ms/step - loss: 1.8103 - accuracy: 0.4439\nEpoch 15/50\n108/108 [==============================] - 2s 21ms/step - loss: 1.6715 - accuracy: 0.4940\nEpoch 16/50\n108/108 [==============================] - 2s 21ms/step - loss: 1.5871 - accuracy: 0.5228\nEpoch 17/50\n108/108 [==============================] - 2s 21ms/step - loss: 1.4711 - accuracy: 0.5435\nEpoch 18/50\n108/108 [==============================] - 2s 20ms/step - loss: 1.4321 - accuracy: 0.5571\nEpoch 19/50\n108/108 [==============================] - 2s 20ms/step - loss: 1.3743 - accuracy: 0.5634\nEpoch 20/50\n108/108 [==============================] - 2s 19ms/step - loss: 1.3333 - accuracy: 0.5808\nEpoch 21/50\n108/108 [==============================] - 2s 19ms/step - loss: 1.2832 - accuracy: 0.5878\nEpoch 22/50\n108/108 [==============================] - 2s 20ms/step - loss: 1.2028 - accuracy: 0.6055\nEpoch 23/50\n108/108 [==============================] - 2s 19ms/step - loss: 1.1736 - accuracy: 0.6210\nEpoch 24/50\n108/108 [==============================] - 2s 20ms/step - loss: 1.1260 - accuracy: 0.6389\nEpoch 25/50\n108/108 [==============================] - 2s 22ms/step - loss: 1.1108 - accuracy: 0.6424\nEpoch 26/50\n108/108 [==============================] - 2s 20ms/step - loss: 1.0858 - accuracy: 0.6379\nEpoch 27/50\n108/108 [==============================] - 2s 20ms/step - loss: 1.0247 - accuracy: 0.6600\nEpoch 28/50\n108/108 [==============================] - 2s 20ms/step - loss: 1.0278 - accuracy: 0.6532\nEpoch 29/50\n108/108 [==============================] - 2s 22ms/step - loss: 1.0396 - accuracy: 0.6452\nEpoch 30/50\n108/108 [==============================] - 2s 22ms/step - loss: 1.0088 - accuracy: 0.6543\nEpoch 31/50\n108/108 [==============================] - 2s 20ms/step - loss: 1.0044 - accuracy: 0.6654\nEpoch 32/50\n108/108 [==============================] - 2s 21ms/step - loss: 0.9822 - accuracy: 0.6608\nEpoch 33/50\n108/108 [==============================] - 2s 20ms/step - loss: 0.9694 - accuracy: 0.6705\nEpoch 34/50\n108/108 [==============================] - 2s 20ms/step - loss: 0.9291 - accuracy: 0.6739\nEpoch 35/50\n108/108 [==============================] - 2s 19ms/step - loss: 0.9623 - accuracy: 0.6623\nEpoch 36/50\n108/108 [==============================] - 2s 19ms/step - loss: 0.9424 - accuracy: 0.6759\nEpoch 37/50\n108/108 [==============================] - 2s 20ms/step - loss: 0.9438 - accuracy: 0.6638\nEpoch 38/50\n108/108 [==============================] - 2s 20ms/step - loss: 0.9539 - accuracy: 0.6650\nEpoch 39/50\n108/108 [==============================] - 2s 20ms/step - loss: 0.9239 - accuracy: 0.6740\nEpoch 40/50\n108/108 [==============================] - 2s 22ms/step - loss: 0.9397 - accuracy: 0.6484\nEpoch 41/50\n108/108 [==============================] - 2s 20ms/step - loss: 0.9123 - accuracy: 0.6754\nEpoch 42/50\n108/108 [==============================] - 2s 22ms/step - loss: 0.9046 - accuracy: 0.6804\nEpoch 43/50\n108/108 [==============================] - 2s 22ms/step - loss: 0.9038 - accuracy: 0.6721\nEpoch 44/50\n108/108 [==============================] - 2s 21ms/step - loss: 0.8947 - accuracy: 0.6756\nEpoch 45/50\n108/108 [==============================] - 2s 20ms/step - loss: 0.8912 - accuracy: 0.6736\nEpoch 46/50\n108/108 [==============================] - 2s 20ms/step - loss: 0.8802 - accuracy: 0.6840\nEpoch 47/50\n108/108 [==============================] - 2s 21ms/step - loss: 0.8960 - accuracy: 0.6706\nEpoch 48/50\n108/108 [==============================] - 2s 20ms/step - loss: 0.8778 - accuracy: 0.6757\nEpoch 49/50\n108/108 [==============================] - 2s 20ms/step - loss: 0.8805 - accuracy: 0.6753\nEpoch 50/50\n108/108 [==============================] - 2s 20ms/step - loss: 0.8865 - accuracy: 0.6653\n","name":"stdout"},{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fb91425c650>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"poetry_length = 5 # lenght of the returned word '_', number of repetitions\nn_lines = 2 # number of words 'i'\ntest_word = 'awo' # input word # podaj pierwszą literę , początek pseudo word\ntest_word = \" \".join(test_word) # add spaces\n\ntext = []\nfor _ in range(poetry_length):\n    encoded = token.texts_to_sequences([test_word]) # encode into integer vector\n    encoded = pad_sequences(encoded, maxlen=seq_length, padding='pre') # apply padding so we have as long as number features\n#encoded\n    pred = model.predict(encoded) # make prediction, returns probabilities of the next word from dictionary\n    y_pred = np.argmax(pred, axis=-1)[0] # take the highest probability and return\n#y_pred\n#predicted_word = \"\"\n    for word, index in token.word_index.items():\n        if index == y_pred:\n            predicted_word = word\n    test_word = test_word + ' ' + predicted_word\n#text.append(predicted_word\ntest_word","execution_count":148,"outputs":[{"output_type":"execute_result","execution_count":148,"data":{"text/plain":"'a w o r d a ż o'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start defines start of the pseudoword, one or more chars\n# length defines how many chars to add\ndef pseudoword(start, length):\n    temp = \" \".join(start) # add spaces\n    for _ in range(length):\n        encoded = token.texts_to_sequences([temp]) # encode into integer vector\n        encoded = pad_sequences(encoded, maxlen=seq_length, padding='pre') # apply padding so we have as long as number features\n        pred = model.predict(encoded) # make prediction, returns probabilities of the next word from dictionary\n        y_pred = np.argmax(pred, axis=-1)[0] # take the highest probability and return\n        predicted_word = \"\"\n        for word, index in token.word_index.items():\n            if index == y_pred:\n                predicted_word = word\n        temp = temp + ' ' + predicted_word\n    temp = temp.replace(\" \", \"\")\n    print(temp)\n    \n ","execution_count":225,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pseudoword(start = \"pok\", length = 7)  ","execution_count":231,"outputs":[{"output_type":"stream","text":"pokcentiat\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}